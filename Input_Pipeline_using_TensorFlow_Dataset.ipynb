{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rkroZwrBBr2u"
      },
      "outputs": [],
      "source": [
        "# Understanding TensorFlow Input Pipeline and optimizing it.\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will analyse, how to make dataset ingestion (including reading dataset,\n",
        "# preprocessing it and making it available for training)\n",
        "# To create an input pipeline, you must start with a data source.\n",
        "\n",
        "# 1. download the dataset and make it in file\n",
        "data_url = 'https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/train.tfrecord'\n",
        "print(data_url.split('/')[-1])\n",
        "files = tf.keras.utils.get_file(data_url.split('/')[-1], data_url)\n"
      ],
      "metadata": {
        "id": "K5vVQI-58a2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e29c24-0b63-4d83-e1b6-bcdc00c8594b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.tfrecord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a parsing function which will transform each data record\n",
        "def parse_func(record):\n",
        "  # Extracts features and return labels.\n",
        "\n",
        "  # record: File path to a TFRecord file\n",
        "\n",
        "  features = {\n",
        "    \"terms\": tf.io.VarLenFeature(dtype=tf.string),\n",
        "    \"labels\": tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
        "  }\n",
        "\n",
        "  parsed_features = tf.io.parse_single_example(record, features)\n",
        "  labels = parsed_features['labels']\n",
        "\n",
        "  return labels"
      ],
      "metadata": {
        "id": "FjFi8oYSSyHQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow._api.v2.data import AUTOTUNE\n",
        "\n",
        "# It creates as Record to read one or more TFRecords sequentially.\n",
        "# It returns dataset which is a tf.data.Dataset object and It is nothing but Python iterable.\n",
        "dataset = tf.data.TFRecordDataset(files)\n",
        "\n",
        "# We can shuffle the dataset  as It may be coming from various different data sources\n",
        "# this transformation maintains a fixed-size buffer and chooses the next element\n",
        "# uniformly at random from that buffer.\n",
        "dataset = dataset.shuffle(buffer_size=100)\n",
        "\n",
        "# We can apply transformation on dataset. It will act as preprocessign steps on input data.\n",
        "# There are verious transform functions like map, filter, flatmap etc.\n",
        "dataset = dataset.map(lambda record : parse_func(record))\n",
        "\n",
        "# The simplest form of batching stacks n (here = 32) consecutive elements of a dataset into a single element.\n",
        "dataset = dataset.batch(batch_size = 32)\n",
        "\n",
        "# This allows later elements to be prepared while the current element is being processed.\n",
        "# This often improves latency and throughput, at the cost of using additional memory\n",
        "# to store prefetched elements.\n",
        "dataset = dataset.prefetch(buffer_size = AUTOTUNE)\n",
        "\n",
        "# All the steps will run and create respective Dataset objects and will execute it.\n",
        "# Note: Since the buffer_size is 100, and the batch size is 32, the first batch\n",
        "# contains no elements with an index over 132."
      ],
      "metadata": {
        "id": "YepF3jt58eEw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since dataset is Python iterable, we can iterate over the elements of dataset.\n",
        "# Here, Iterator resource is created once and next() instance can be created once and called as many times.\n",
        "# Once, We get out of loop scope, Iterator instance is deleted.\n",
        "# thus, at one iteration a batch of 32 elements will be fetched\n",
        "\n",
        "for record in dataset:\n",
        "  print(record)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14eu8FWTYzY7",
        "outputId": "25adc57e-10d3-4147-840a-ee886ceb77e5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]], shape=(32, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The tf.data API enables you to build complex input pipelines from simple, reusable pieces.\n",
        "# and Now we use GPUs and TPUs that can radically reduce the time required to execute a\n",
        "# single training step because of parallelization\n",
        "\n",
        "# There are some scope of Optimization on above created data pipeline\n",
        "\n",
        "# 1. software pipelining  =>  prefetch(buffer_size = X)\n",
        "# 2. processing parallelization => map(...., num_parallel_calls = N)\n",
        "# 3. I/O parallelization => Interleave (....., num_parallel_calls = N)\n",
        "\n",
        "\n",
        "# lets understand traditional method:\n",
        "# In a naive synchronous implementation like above, while your pipeline is fetching the data,\n",
        "# your transformation model is sitting idle. Conversely, while your model is training,\n",
        "# the input pipeline is sitting idle. The training step time is thus the sum of opening,\n",
        "# reading and training times.\n",
        "\n",
        "'''\n",
        "e.g:\n",
        "\n",
        "1. shuffle  -> takes 20ms.\n",
        "2. map  -> takes 200ms\n",
        "3. filter  -> takes 300ms\n",
        "\n",
        "So, total time It will take to process all is => 20 + 200 + 300 = 520ms\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "gtcSCsFTaF74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On Contrary, If we use parallelization in transformation functions\n",
        "\n",
        "'''\n",
        "e.g :\n",
        "1. shuffle  -> 20ms\n",
        "2. map(..., num_parallel_calls = 1)  -> takes 200ms\n",
        "3. filter(...., num_parallel_calls = 1)  -> takes 300ms\n",
        "\n",
        "Here, total time to process all function is 300ms as all other functions will run in parallel threads.\n",
        "\n",
        "'''\n",
        "\n",
        "# Therefore, num_parallel_calls can help reduce the time consume for overall computation.\n",
        "\"\"\"\n",
        "Choosing the best value for the num_parallel_calls argument depends on your hardware,\n",
        "characteristics of your training data (such as its size and shape)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# But, A simple heuristic is to use the number of available CPU cores.\n",
        "# We can use num_parallel_calls = AUTOTUNE, which will delegate the decision\n",
        "# about what level of parallelism to use to the tf.data runtime."
      ],
      "metadata": {
        "id": "Q6k74D2cdz5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caching : tf.data.Dataset.cache\n",
        "# The tf.data.Dataset.cache transformation can cache a dataset, either in memory\n",
        "# or on local storage. This will save some operations (like file opening and data reading)\n",
        "# from being executed during each epoch.\n",
        "# Note: Apply caching just after time consuming operations (like map transform or reading file)\n",
        "\n",
        "\"\"\"\n",
        "If the user-defined function passed into the map transformation is expensive, apply\n",
        " the cache transformation after the map transformation as long as the resulting\n",
        " dataset can still fit into memory or local storage.\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "V_311NzTf8yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prefetch  : tf.data.Dataset.prefetch\n",
        "# Prefetching overlaps the preprocessing and model execution of a training step.\n",
        "# While the model is executing training step s, the input pipeline is reading the data for step s+1.\n",
        "\n",
        "\"\"\"\n",
        " It can be used to decouple the time when data is produced from the time when data is consumed.\n",
        " In particular, the transformation uses a background thread and an internal buffer to\n",
        " prefetch elements from the input dataset ahead of the time they are requested.\n",
        "\n",
        " The number of elements to prefetch should be equal to (or possibly greater than)\n",
        " the number of batches consumed by a single training step. (or AUTOTUNE)\n",
        "\n",
        " \"\"\"\n"
      ],
      "metadata": {
        "id": "scRRBkgB548u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interleave  : tf.data.Dataset.interleave\n",
        "# Its a technique of parallelizing data extraction.\n",
        "\n",
        "\"\"\"\n",
        "A dataset pipeline that works well when reading data locally might become bottlenecked\n",
        "on I/O when reading data remotely because of the following differences between local and remote storage:\n",
        "\n",
        "1. Time-to-first-byte: Reading the first byte of a file from remote storage can take\n",
        "orders of magnitude longer than from local storage.\n",
        "\n",
        "2. Read throughput: While remote storage typically offers large aggregate bandwidth,\n",
        " reading a single file might only be able to utilize a small fraction of this bandwidth.\n",
        " \"\"\"\n",
        "\n",
        "# To mitigate the impact of the various data extraction overheads,\n",
        "# the tf.data.Dataset.interleave transformation can be used to parallelize the data loading step,\n",
        "# interleaving the contents of other datasets (such as data file readers).\n",
        "\n",
        "filenames = [\"/data/file1.txt\", \"/data/file2.txt\",\n",
        "             \"/data/file3.txt\", \"/data/file4.txt\"]   # set your own data path\n",
        "dataset_tmp = tf.data.Dataset.from_tensor_slices(filenames)\n",
        "dataset_tmp = dataset_tmp.interleave(lambda x: tf.data.TFRecordDataset(x),\n",
        "    cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    deterministic=False)"
      ],
      "metadata": {
        "id": "sEKgiArV68kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducing memory footprint\n",
        "# A number of transformations, including interleave, prefetch, and shuffle, maintain\n",
        "# an internal buffer of elements.\n",
        "\n",
        "\"\"\"\n",
        " If the user-defined function passed into the map transformation changes the size of the elements,\n",
        "  then the ordering of the map transformation and the transformations that buffer elements\n",
        "  affects the memory usage. In general, choose the order that results in lower memory footprint,\n",
        "  unless different ordering is desirable for performance.\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "6Zu5OakMgX2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As per Tensorflow official documentation page\n",
        "\n",
        "\"\"\"\n",
        "Here is a summary of the best practices for designing performant TensorFlow input pipelines:\n",
        "\n",
        "-> Use the prefetch transformation to overlap the work of a producer and consumer\n",
        "-> Parallelize the data reading transformation using the interleave transformation\n",
        "-> Parallelize the map transformation by setting the num_parallel_calls argument\n",
        "-> Use the cache transformation to cache data in memory during the first epoch\n",
        "-> Vectorize user-defined functions passed in to the map transformation\n",
        "-> Reduce memory usage when applying the interleave, prefetch, and shuffle transformations\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iRAxl2sIgl26"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}